{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sshtunnel import SSHTunnelForwarder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"misc/.env\")\n",
    "\n",
    "# SSH and DB configuration from environment variables\n",
    "SSH_HOST = os.getenv(\"SSH_HOST\")\n",
    "SSH_PORT = int(os.getenv(\"SSH_PORT\", 22))\n",
    "SSH_USER = os.getenv(\"SSH_USER\")\n",
    "SSH_PASSWORD = os.getenv(\"SSH_PASSWORD\")\n",
    "\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = int(os.getenv(\"DB_PORT\", 5432))\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def __create_ssh_tunnel():\n",
    "\n",
    "    tunnel = None\n",
    "    try:\n",
    "\n",
    "        tunnel = SSHTunnelForwarder(\n",
    "            (SSH_HOST, SSH_PORT),\n",
    "            ssh_username=SSH_USER,\n",
    "            ssh_password=SSH_PASSWORD,\n",
    "            remote_bind_address=(DB_HOST, DB_PORT),\n",
    "            local_bind_address=(\"127.0.0.1\", 5432),\n",
    "        )\n",
    "        tunnel.start()\n",
    "\n",
    "        yield tunnel\n",
    "    except Exception as e:\n",
    "\n",
    "        raise\n",
    "    finally:\n",
    "        if tunnel:\n",
    "            tunnel.stop()\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def create_db_engine():\n",
    "    \"\"\"\n",
    "    Creates an SQLAlchemy engine connected through the SSH tunnel.\n",
    "    Yields the engine for use in a with statement.\n",
    "    \"\"\"\n",
    "    with __create_ssh_tunnel() as tunnel:\n",
    "        engine = None\n",
    "        try:\n",
    "\n",
    "            connection_string = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{tunnel.local_bind_port}/{DB_NAME}\"\n",
    "\n",
    "            engine = create_engine(\n",
    "                connection_string, echo=False, pool_pre_ping=True, pool_recycle=3600\n",
    "            )\n",
    "            yield engine\n",
    "        except SQLAlchemyError as e:\n",
    "\n",
    "            raise\n",
    "        finally:\n",
    "            if engine:\n",
    "                engine.dispose()\n",
    "\n",
    "\n",
    "def create_crowd_levels(df, target_district):\n",
    "    target_column = f'{target_district.replace(\"_\", \" \")}_c_lvl'\n",
    "\n",
    "    out = pd.qcut(\n",
    "        df[target_district].rank(method=\"first\"),\n",
    "        q=[0, 0.3, 0.7, 1],  # q=[0, 0.2, 0.45, 0.65, 0.8, 1] for 5 bins\n",
    "        labels=[0, 1, 2],\n",
    "    )\n",
    "\n",
    "    return out.astype(np.uint8), target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with create_db_engine() as engine:\n",
    "    with engine.connect() as conn:\n",
    "        query = f\"SELECT * FROM crowdedness\"\n",
    "        chunks = pd.read_sql_query(sql=query, con=conn, chunksize=10**6)\n",
    "    df = pd.concat(chunks, ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.pivot_table(\n",
    "        index=\"timestamp\",\n",
    "        columns=\"district_id\",\n",
    "        values=\"crowd\",\n",
    "        aggfunc=\"sum\",\n",
    "    )\n",
    "    .ffill()\n",
    "    .bfill()\n",
    "    .astype(np.uint16)\n",
    "    .sort_values(by=\"timestamp\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df.set_index(\"timestamp\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [col for col in df.columns if df[col].var() > 0]\n",
    "filtered_df = df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bedrijvenpark Noord-West\n",
      "[0 1 2]\n",
      "Bedrijventerrein Schieveen\n",
      "[0 1 2]\n",
      "Charlois\n",
      "[0 1 2]\n",
      "Delfshaven\n",
      "[0 1 2]\n",
      "Feijenoord\n",
      "[0 1 2]\n",
      "Hillegersberg-Schiebroek\n",
      "[0 1 2]\n",
      "Hoek van Holland\n",
      "[0 1 2]\n",
      "Hoogvliet\n",
      "[0 1 2]\n",
      "Kralingen-Crooswijk\n",
      "[0 1 2]\n",
      "Nieuw Mathenesse\n",
      "[0 1 2]\n",
      "Noord\n",
      "[0 1 2]\n",
      "Overschie\n",
      "[0 1 2]\n",
      "Pernis\n",
      "[0 1 2]\n",
      "Prins Alexander\n",
      "[0 1 2]\n",
      "Rivium\n",
      "[0 1 2]\n",
      "Rotterdam Centrum\n",
      "[0 1 2]\n",
      "Spaanse Polder\n",
      "[0 1 2]\n",
      "Vondelingenplaat\n",
      "[0 1 2]\n",
      "Waalhaven\n",
      "[0 1 2]\n",
      "Ä²sselmonde\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "target_columns = {}\n",
    "for target_district in filtered_df.columns:\n",
    "    print(target_district)\n",
    "    out, target_column = create_crowd_levels(df, target_district)\n",
    "    print(np.unique(out))\n",
    "    # if diff := set([0, 1, 2]) - set(out):\n",
    "    #     diff = diff.pop()\n",
    "    #     print(diff)\n",
    "    #     encoder = OrdinalEncoder(\n",
    "    #         categories=[[0, 1, 2]],\n",
    "    #         handle_unknown=\"use_encoded_value\",\n",
    "    #         unknown_value=diff,\n",
    "    #     )\n",
    "    #     out = encoder.fit_transform(out)\n",
    "    #     print(np.unique(out))\n",
    "    # target_columns[target_column] = out\n",
    "# target_labels = list(target_columns.keys())\n",
    "# df[target_labels] = pd.DataFrame(target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_dataset(lagged_df, n_targets, district, step):\n",
    "    temp = lagged_df.copy(deep=True)\n",
    "    temp[district] = temp[district].shift(-step)\n",
    "    temp = temp.dropna()\n",
    "    temp[district] = temp[district].astype(np.uint8)\n",
    "\n",
    "    X = temp[temp.columns[:-n_targets]]\n",
    "    y = temp[district]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab-bvit9Y5v-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
