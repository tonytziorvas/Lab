{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = len(os.listdir(\"data/raw\")) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l 'data/raw/extracted/week_{n_files}.csv.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = f\"data/raw/week_{n_files}.csv\"\n",
    "city = \"rotterdam\"\n",
    "ts_col = \"ts\"\n",
    "chunk_size = 10**7\n",
    "n_chunks = int(173_453_671 / chunk_size) + 1\n",
    "dtypes = {\n",
    "    \"form_factor\": \"object\",\n",
    "    \"system_id\": \"object\",\n",
    "    \"longitude\": \"float64\",\n",
    "    \"latitude\": \"float64\",\n",
    "    \"ts\": \"int64\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Data Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Netherlands polygon from the Natural Earth dataset\n",
    "\n",
    "world_filepath = gpd.datasets.get_path(\"naturalearth_lowres\")\n",
    "world = gpd.read_file(world_filepath)\n",
    "netherlands = world.loc[world[\"name\"] == \"Netherlands\"]\n",
    "\n",
    "city_boundaries = gpd.read_file(f\"data/boundaries/{city}_.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read directly from zip w/out extracting to csv manually\n",
    "# READ_ZIP = True\n",
    "\n",
    "# if READ_ZIP:\n",
    "#     zip_path = \"data/raw/data.zip\"\n",
    "#     csv_name = \"data/datanew.csv\"\n",
    "\n",
    "#     df = helper.read_zip(zip_path, csv_name)\n",
    "# else:\n",
    "#     df = pd.read_csv(\"data/raw/aprox1week.csv\").drop_duplicates()\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "i = 0\n",
    "\n",
    "processed_chunks = []\n",
    "\n",
    "with pd.read_csv(FILE_PATH, chunksize=chunk_size, dtype=dtypes) as chunks:\n",
    "    for chunk in tqdm(chunks, \"Processing Chunks\", total=n_chunks):\n",
    "        chunk = chunk.drop_duplicates().reset_index(drop=True)\n",
    "        chunk[(chunk[\"longitude\"] != 0) & (chunk[\"latitude\"] != 0)]\n",
    "        geometry = gpd.points_from_xy(chunk.longitude, chunk.latitude, crs=4326)\n",
    "        chunk = gpd.GeoDataFrame(chunk, geometry=geometry)\n",
    "\n",
    "        df_left = pd.DataFrame(\n",
    "            data=chunk.sindex.query(city_boundaries.geometry, predicate=\"intersects\").T,\n",
    "            columns=[\"district_id\", \"point_id\"],\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        df_right = (\n",
    "            chunk.iloc[df_left[\"point_id\"]][ts_col]\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\": \"point_id\", ts_col: \"timestamp\"})\n",
    "        )\n",
    "\n",
    "        points = pd.merge(df_left, df_right, on=\"point_id\")\n",
    "\n",
    "        points = pd.merge(\n",
    "            points,\n",
    "            chunk[[\"latitude\", \"longitude\"]],\n",
    "            left_on=\"point_id\",\n",
    "            right_index=True,\n",
    "        )\n",
    "\n",
    "        # Map district_id to district names\n",
    "        district_codes = dict(city_boundaries.iloc[points.district_id.unique()][\"name\"])\n",
    "        points[\"district_id\"] = points[\"district_id\"].map(district_codes)\n",
    "        processed_chunks.append(points)\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = (\n",
    "    pd.concat(processed_chunks, ignore_index=True)\n",
    "    .sort_values(by=\"point_id\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data contains {gdf.timestamp.nunique()} timestamps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_center = (gdf.latitude.mean(), gdf.longitude.mean())\n",
    "map = folium.Map(location=map_center, zoom_start=10)\n",
    "folium.TileLayer(\"openstreetmap\").add_to(map)\n",
    "\n",
    "# Add points and polygons as GeoJSON overlays\n",
    "for idx, row in gdf.sample(10000).iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row.latitude, row.longitude],\n",
    "        radius=2,  # Radius of the circle marker\n",
    "        color=\"red\",  # Color of the marker border\n",
    "        fill=True,\n",
    "        fill_color=\"red\",  # Color of the marker fill\n",
    "        fill_opacity=0.6,  # Opacity of the marker fill\n",
    "        popup=row[\"name\"] if \"name\" in row else None,  # Optional popup text\n",
    "    ).add_to(map)\n",
    "\n",
    "folium.GeoJson(city_boundaries.geometry).add_to(map)\n",
    "\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Data Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_per_district = (\n",
    "    gdf.groupby(by=[\"district_id\", \"timestamp\"])\n",
    "    .agg({\"point_id\": \"count\"})\n",
    "    .rename({\"point_id\": \"crowd\"}, axis=1)\n",
    "    .sort_values(by=\"crowd\", ascending=False)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "points_per_district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_per_district.to_parquet(\n",
    "    f\"data/processed/points_per_district_week_{n_files}.parquet\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crowd Analysis\n",
    "\n",
    "Let's delve deeper and focus on the most crowded district to see how the crowdedness evolves over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_crowded_district_idx = (\n",
    "    points_per_district.groupby([\"district_id\"])\n",
    "    .sum()\n",
    "    .sort_values(by=\"crowd\", ascending=False)\n",
    "    .reset_index()\n",
    "    .iloc[0][\"district_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_crowded_district = (\n",
    "    points_per_district.loc[\n",
    "        points_per_district[\"district_id\"] == most_crowded_district_idx\n",
    "    ]\n",
    "    .drop(columns=\"district_id\")\n",
    "    .sort_values(by=\"timestamp\")\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(13, 6))\n",
    "\n",
    "\n",
    "ax.plot(\n",
    "    pd.to_datetime(most_crowded_district[\"timestamp\"], unit=\"s\"),\n",
    "    most_crowded_district[\"crowd\"],\n",
    "    linestyle=\"-\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    f\"Crowdedness Over Time in District {most_crowded_district_idx}\", fontsize=18\n",
    ")\n",
    "ax.set_xlabel(\"Timestamp\", fontsize=12)\n",
    "ax.set_ylabel(\"No. of Points\", fontsize=12)\n",
    "plt.gca().spines[[\"top\", \"right\"]].set_visible(False)\n",
    "ax.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d--%H:%M\"))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/most_crowded_week_{n_files}.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_crowded_district.to_parquet(\n",
    "    f\"data/processed/most_crowded_week_{n_files}.parquet\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Data Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_data = [\n",
    "    pd.read_parquet(f\"data/processed/{week}\")\n",
    "    for week in os.listdir(\"data/processed\")\n",
    "    if week.startswith(\"points_\")\n",
    "]\n",
    "\n",
    "\n",
    "pd.concat(weekly_data, ignore_index=True).to_parquet(\n",
    "    \"data/final/points_per_district_full.parquet.gzip\", compression=\"gzip\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_t, max_t = pd.to_datetime(df[\"timestamp\"].apply([\"min\", \"max\"]), unit=\"s\").apply(\n",
    "    lambda x: x.strftime(\"%d-%m-%Y--%H:%M\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is gathered between 18-06-2024--06:21 and 04-07-2024--08:30\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'helper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData is gathered between \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_t\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_t\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime between entries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhelper\u001b[38;5;241m.\u001b[39mREFRESH_INTERVAL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'helper' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Data is gathered between {min_t} and {max_t}\")\n",
    "print(f\"Time between entries: {helper.REFRESH_INTERVAL} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"rotterdam\"\n",
    "ts_col = \"ts\"\n",
    "chunk_size = 10**7\n",
    "dtypes = {\n",
    "    \"form_factor\": \"object\",\n",
    "    \"system_id\": \"object\",\n",
    "    \"longitude\": \"float64\",\n",
    "    \"latitude\": \"float64\",\n",
    "    \"ts\": \"int64\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5_/7z8lzq913cl3q8m0z5pnmpjr0000gn/T/ipykernel_18560/2494278835.py:3: FutureWarning: The geopandas.dataset module is deprecated and will be removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.\n",
      "  world_filepath = gpd.datasets.get_path(\"naturalearth_lowres\")\n"
     ]
    }
   ],
   "source": [
    "# Load the Netherlands polygon from the Natural Earth dataset\n",
    "\n",
    "world_filepath = gpd.datasets.get_path(\"naturalearth_lowres\")\n",
    "world = gpd.read_file(world_filepath)\n",
    "netherlands = world.loc[world[\"name\"] == \"Netherlands\"]\n",
    "\n",
    "city_boundaries = gpd.read_file(f\"data/boundaries/{city}_.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_files(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "\n",
    "def count_rows_in_csv(file_path):\n",
    "    result = subprocess.run([\"wc\", \"-l\", file_path], capture_output=True, text=True)\n",
    "    return int(result.stdout.split()[0]) - 1  # Subtract 1 for header row\n",
    "\n",
    "\n",
    "def _process_csv(\n",
    "    file, chunk_size=chunk_size, dtypes=None, filter_func=None, n_chunks=None\n",
    "):\n",
    "    tqdm.pandas()\n",
    "    all_chunks = []\n",
    "    with pd.read_csv(file, chunksize=chunk_size, dtype=dtypes) as chunks:\n",
    "        for chunk in tqdm(chunks, desc=\"    Processing Chunks...\", total=n_chunks):\n",
    "            if filter_func:\n",
    "                chunk = filter_func(chunk)\n",
    "            all_chunks.append(chunk)\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def my_filter_func(chunk):\n",
    "    processed_chunks = []\n",
    "\n",
    "    chunk = chunk.drop_duplicates().reset_index(drop=True)\n",
    "    chunk[(chunk[\"longitude\"] != 0) & (chunk[\"latitude\"] != 0)]\n",
    "    geometry = gpd.points_from_xy(chunk.longitude, chunk.latitude, crs=4326)\n",
    "    chunk = gpd.GeoDataFrame(chunk, geometry=geometry)\n",
    "\n",
    "    df_left = pd.DataFrame(\n",
    "        data=chunk.sindex.query(city_boundaries.geometry, predicate=\"intersects\").T,\n",
    "        columns=[\"district_id\", \"point_id\"],\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    df_right = (\n",
    "        chunk.iloc[df_left[\"point_id\"]][ts_col]\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"point_id\", ts_col: \"timestamp\"})\n",
    "    )\n",
    "    merged = pd.merge(df_left, df_right, on=\"point_id\")\n",
    "    merged = pd.merge(\n",
    "        merged,\n",
    "        chunk[[\"latitude\", \"longitude\"]],\n",
    "        left_on=\"point_id\",\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Map district_id to district names\n",
    "    district_codes = dict(city_boundaries.iloc[merged.district_id.unique()][\"name\"])\n",
    "    merged[\"district_id\"] = merged[\"district_id\"].map(district_codes)\n",
    "    processed_chunks.append(merged)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def save_to_parquet(df, output_path, compression=\"gzip\"):\n",
    "    print(f\"    Saving to {output_path}...\")\n",
    "    df.to_parquet(output_path, compression=compression)\n",
    "\n",
    "\n",
    "def process_csv_zip(\n",
    "    zip_path,\n",
    "    filter_func=None,\n",
    "    output_path=\"output.parquet\",\n",
    "    chunk_size=100000,\n",
    "    dtypes=None,\n",
    "):\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    try:\n",
    "        print(f\"Unzipping {zip_path} to {temp_dir}\")\n",
    "        unzip_files(zip_path, temp_dir)\n",
    "\n",
    "        for csv_file in os.listdir(temp_dir):\n",
    "            if csv_file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(temp_dir, csv_file)\n",
    "                print(f\"Processing {csv_file}\")\n",
    "                n_rows = count_rows_in_csv(file_path)\n",
    "                n_chunks = (n_rows // chunk_size) + 1\n",
    "                chunks = _process_csv(\n",
    "                    file_path, chunk_size, dtypes, filter_func, n_chunks\n",
    "                )\n",
    "        processed_df = pd.concat(chunks)\n",
    "\n",
    "        save_to_parquet(processed_df, output_path)\n",
    "    finally:\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"    Moving {zip_path} to data/raw/extracted\")\n",
    "        shutil.move(zip_path, \"data/raw/extracted\")\n",
    "\n",
    "\n",
    "def etl_pipeline():\n",
    "    n_week = 1\n",
    "\n",
    "    # Load all csv files\n",
    "    zip_paths = sorted(\n",
    "        [\n",
    "            os.path.join(\"data/raw\", file)\n",
    "            for file in os.listdir(\"data/raw\")\n",
    "            if file.endswith(\".csv.zip\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Process each csv file sequentially\n",
    "    for zip_path in zip_paths:\n",
    "        output = f\"data/processed/points_per_district_week_{n_week}.parquet\"\n",
    "\n",
    "        process_csv_zip(\n",
    "            zip_path,\n",
    "            filter_func=my_filter_func,\n",
    "            output_path=output,\n",
    "            chunk_size=chunk_size,\n",
    "            dtypes=dtypes,\n",
    "        )\n",
    "\n",
    "        n_week += 1\n",
    "\n",
    "    # Merge all processed parquet files\n",
    "    weekly_data = [\n",
    "        pd.read_parquet(f\"data/processed/{week}\")\n",
    "        for week in os.listdir(\"data/processed\")\n",
    "        if week.startswith(\"points_\")\n",
    "    ]\n",
    "\n",
    "    # Concat each dataframe and save to `final` folder\n",
    "    print(\"Concatenating dataframes...\")\n",
    "    final_df = (\n",
    "        pd.concat(weekly_data, ignore_index=True)\n",
    "        .groupby(by=[\"district_id\", \"timestamp\"])\n",
    "        .agg({\"point_id\": \"count\"})\n",
    "        .rename({\"point_id\": \"crowd\"}, axis=1)\n",
    "        .sort_values(by=\"crowd\", ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    final_df.to_parquet(\"data/final/points_per_district_full.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping data/raw/week_1.csv.zip to /var/folders/5_/7z8lzq913cl3q8m0z5pnmpjr0000gn/T/tmp_eftr_ji\n",
      "Processing week_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Processing Chunks...: 100%|██████████| 21/21 [04:00<00:00, 11.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving week_1.csv to data/processed/points_per_district_week_1.parquet\n",
      "Saving to data/processed/points_per_district_week_1.parquet...\n",
      "Moving data/raw/week_1.csv.zip to data/raw/extracted\n",
      "Unzipping data/raw/week_2.csv.zip to /var/folders/5_/7z8lzq913cl3q8m0z5pnmpjr0000gn/T/tmpgem4j7fv\n",
      "Processing week_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Processing Chunks...: 100%|██████████| 18/18 [03:24<00:00, 11.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving week_2.csv to data/processed/points_per_district_week_2.parquet\n",
      "Saving to data/processed/points_per_district_week_2.parquet...\n",
      "Moving data/raw/week_2.csv.zip to data/raw/extracted\n",
      "Unzipping data/raw/week_3or4days.csv.zip to /var/folders/5_/7z8lzq913cl3q8m0z5pnmpjr0000gn/T/tmp7koon78_\n",
      "Processing 3or4days.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Processing Chunks...: 100%|██████████| 9/9 [01:37<00:00, 10.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving __MACOSX to data/processed/points_per_district_week_3.parquet\n",
      "Saving to data/processed/points_per_district_week_3.parquet...\n",
      "Moving data/raw/week_3or4days.csv.zip to data/raw/extracted\n",
      "Concatenating dataframes...\n"
     ]
    }
   ],
   "source": [
    "etl_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab-bvit9Y5v-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
